{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a392444-7e4a-4f27-b7e3-8f42dff9d96d",
   "metadata": {},
   "source": [
    "# **Building K means Cluster Algorithms from Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ddc85-9526-4455-8fcd-261888b8d5f2",
   "metadata": {},
   "source": [
    "### Hemant Thapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8d272-d235-4b44-99a5-7d1d91159aea",
   "metadata": {},
   "source": [
    "K-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into a predetermined number of clusters. The main goal of K-means is to group data points into clusters in such a way that points within the same cluster are more similar to each other compared to points in different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53953c7c-0ccd-4517-aef2-666ad07f92f8",
   "metadata": {},
   "source": [
    "- Centroids: Centroids are the representative points of each cluster. They are updated iteratively to minimise the distance between data points and centroids within the same cluster.\n",
    "\n",
    "- Cluster Assignment: Data points are assigned to the cluster with the nearest centroid based on some distance metric, commonly Euclidean distance. This step ensures that data points within the same cluster are similar to each other.\n",
    "\n",
    "- Objective Function: K-means aims to minimise the total within-cluster variance, also known as the inertia or sum of squared distances from each data point to its assigned centroid.\n",
    "\n",
    "- Number of Clusters (K): The number of clusters is a hyperparameter that needs to be specified by the user before running the algorithm. Determining the optimal value of K is often challenging and may require domain knowledge or additional techniques, such as the elbow method or silhouette analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0658b85-e118-4d10-9e1b-c3806d1a1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee2846-af5a-4b73-815b-87562fd1c294",
   "metadata": {},
   "source": [
    "1. **Euclidean Distance**:\n",
    "   The Euclidean distance between two points $x$ and $y$ in an $n$-dimensional space is calculated as follows:\n",
    "   $$ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "   where $x_i$ and $y_i$ are the $i$-th dimensions of points $x$ and $y$ respectively.\n",
    "\n",
    "2. **Centroid Update**:\n",
    "   After assigning data points to clusters, the centroids are updated to the mean of all data points assigned to each cluster. For a cluster $C_k$, the centroid $\\mu_k$ is updated as follows:\n",
    "   $$ \\mu_k = \\frac{1}{|C_k|} \\sum_{x \\in C_k} x $$\n",
    "   where $|C_k|$ denotes the number of data points in cluster $C_k$.\n",
    "\n",
    "3. **Cluster Assignment**:\n",
    "   Data points are assigned to the nearest centroid based on some distance metric, commonly Euclidean distance. This step ensures that data points within the same cluster are similar to each other.\n",
    "\n",
    "4. **Objective Function (Inertia)**:\n",
    "   The objective of K-means clustering is to minimize the total within-cluster variance, also known as \"inertia\" or \"sum of squared distances\" from each data point to its assigned centroid. Mathematically, the inertia $J$ is calculated as follows:\n",
    "   $$ J = \\sum_{k=1}^{K} \\sum_{x \\in C_k} ||x - \\mu_k||^2 $$\n",
    "   where $K$ is the number of clusters, $C_k$ is the $k$-th cluster, $x$ is a data point in cluster $C_k$, and $\\mu_k$ is the centroid of cluster $C_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c58aa1-4007-4426-8ff2-26bc6e0f6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating KMeans\n",
    "class K_Means:\n",
    "    def __init__(self, k=3, tolerance=0.0001, max_iterations=500):\n",
    "        # the number of clusters to form as well as the number of centroids to generate.\n",
    "        self.k = k\n",
    "        # the percentage change in the centroids before considering the algorithm has converged.\n",
    "        self.tolerance = tolerance\n",
    "        # the maximum number of times the algorithm will attempt to converge before stopping.\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    # method to fit model inside dataset\n",
    "    def fit(self, data):\n",
    "        # initialise Centroids Randomly\n",
    "        # empty dictonary to store centroids\n",
    "        self.centroids = {}\n",
    "        # for loop to iterate inside k\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "        # loop through max iterations times\n",
    "        for i in range(self.max_iterations):\n",
    "            # create empty classes for each centroid\n",
    "            self.classes = {i: [] for i in range(self.k)}\n",
    "            # assigning data points to nearest centroid\n",
    "            for features in data:\n",
    "                # euclidean distance between the current data point and each centroid,\n",
    "                distances = [np.linalg.norm(features - self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classes[classification].append(features)\n",
    "            # previous centroids\n",
    "            previous = dict(self.centroids)\n",
    "            # update centroids as mean of assigned data points\n",
    "            for classification in self.classes:\n",
    "                self.centroids[classification] = np.mean(self.classes[classification], axis=0)\n",
    "            # convergence\n",
    "            isOptimal = True\n",
    "            for centroid in self.centroids:\n",
    "                original_centroid = previous[centroid]\n",
    "                curr = self.centroids[centroid]\n",
    "                if np.sum((curr - original_centroid) / original_centroid * 100.0) > self.tolerance:\n",
    "                    isOptimal = False\n",
    "            # break loop if converged\n",
    "            if isOptimal:\n",
    "                break\n",
    "    # method to predict\n",
    "    def prediction(self, data):\n",
    "        # euclidean distance\n",
    "        distances = [np.linalg.norm(data - self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "\n",
    "def within_cluster_sum_of_squares(data, k):\n",
    "    kmeans = K_Means(k)\n",
    "    kmeans.fit(data)\n",
    "    wcss = sum([np.sum([np.linalg.norm(x - kmeans.centroids[ci])**2 for x in kmeans.classes[ci]]) for ci in kmeans.classes])\n",
    "    return wcss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d9c06-4630-429d-8f4c-6d3cfc2c8c21",
   "metadata": {},
   "source": [
    "Dataset with 2 features (columns) and 100 data points (rows), we will illustrate the process with a smaller subset for clarity.\n",
    "\n",
    "#### 1. Euclidean Distance Calculation\n",
    "\n",
    "Let's assume we want to calculate the Euclidean distance between two points $x = (x_1, x_2)$ and $y = (y_1, y_2)$. For example, $x = (2, 3)$ and $y = (5, 7)$.\n",
    "\n",
    "The formula for Euclidean distance is:\n",
    "$$\n",
    "d(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}\n",
    "$$\n",
    "\n",
    "Plugging in our values:\n",
    "$$\n",
    "d(x, y) = \\sqrt{(2 - 5)^2 + (3 - 7)^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
    "$$\n",
    "\n",
    "#### 2. Centroid Update Calculation\n",
    "\n",
    "Assume we have a cluster $C$ with points. To update the centroid $\\mu$ for $C$, we calculate the mean of all points in $C$.\n",
    "\n",
    "For simplicity, let's say $C$ has 3 points: $(2,3)$, $(4,2)$, and $(1,5)$. The updated centroid $\\mu$ is:\n",
    "$$\n",
    "\\mu = \\left( \\frac{2+4+1}{3}, \\frac{3+2+5}{3} \\right) = \\left( \\frac{7}{3}, \\frac{10}{3} \\right) = (2.33, 3.33)\n",
    "$$\n",
    "\n",
    "#### 3. Cluster Assignment\n",
    "\n",
    "For a point $x = (4, 5)$ and given centroids $\\mu_1 = (2, 3)$ and $\\mu_2 = (6, 7)$, we calculate the Euclidean distance to each centroid and assign $x$ to the nearest one.\n",
    "\n",
    "- To $\\mu_1$: $d(x, \\mu_1) = \\sqrt{(4 - 2)^2 + (5 - 3)^2} = \\sqrt{8}$\n",
    "- To $\\mu_2$: $d(x, \\mu_2) = \\sqrt{(4 - 6)^2 + (5 - 7)^2} = \\sqrt{8}$\n",
    "\n",
    "#### 4. Objective Function (Inertia) Calculation\n",
    "\n",
    "Considering a simplified example with clusters $C_1$ and $C_2$ after assignment:\n",
    "\n",
    "- $C_1$ with points $(2,3)$ and its centroid $\\mu_1 = (2, 3)$,\n",
    "- $C_2$ with points $(4,5)$, $(6,7)$ and its centroid $\\mu_2 = (5, 6)$.\n",
    "\n",
    "The inertia $J$ is calculated as:\n",
    "$$\n",
    "J = \\sum_{k=1}^{2} \\sum_{x \\in C_k} ||x - \\mu_k||^2\n",
    "$$\n",
    "\n",
    "For $C_1$, since the only point is the centroid, the contribution to $J$ is $0$.\n",
    "\n",
    "For $C_2$:\n",
    "- For $(4,5)$ to $\\mu_2$: $||x - \\mu_k||^2 = 2$\n",
    "- For $(6,7)$ to $\\mu_2$: $||x - \\mu_k||^2 = 2$\n",
    "\n",
    "Thus, $J = 4$.\n",
    "\n",
    "To apply these steps to your full dataset with 100 rows, you would:\n",
    "- Calculate the Euclidean distance between each data point and each centroid to perform cluster assignments.\n",
    "- Update centroids after assignment by calculating the mean of all points within each cluster.\n",
    "- Calculate the objective function to evaluate the clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a01dbb-9618-4dcf-b73c-c92748ddd5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating dataset for testing\n",
    "np.random.seed(42)\n",
    "# creating data from random numbers\n",
    "data = np.random.randn(100, 2)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60168839-0229-47c9-ae3f-2ddd7af522f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K_Means for the clustering\n",
    "km = K_Means(3)\n",
    "# fitting model\n",
    "km.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88ddf9-eed1-4a83-9426-9e776495a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking centroids\n",
    "km.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2950e7-88c4-4b67-8298-cea34610e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "def clustering(km, x1, x2):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # total colors for plotting clusters\n",
    "    colors = [\"black\", \"grey\", \"cyan\", \"green\", \"blue\", \"magenta\", \"yellow\", \"white\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "\n",
    "    # iterate over each cluster\n",
    "    for classification in km.classes:\n",
    "        color = colors[classification]\n",
    "        for features in km.classes[classification]:\n",
    "            # plotting each data point in the cluster\n",
    "            plt.scatter(features[0], features[1], color=color, s=30, alpha=0.5)\n",
    "\n",
    "    # initialise centroid label counter\n",
    "    centroid_label = 1\n",
    "\n",
    "    # iterate over each centroid and plot them with number labels\n",
    "    for centroid in km.centroids:\n",
    "        # plotting centroid of cluster\n",
    "        plt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], s=300, marker=\"X\", color='red')\n",
    "        # text label at centroid center\n",
    "        plt.text(km.centroids[centroid][0], km.centroids[centroid][1], str(centroid_label), ha='center', va='center', color='white', fontsize=12, fontweight='bold')\n",
    "        # increment centroid label counter\n",
    "        centroid_label += 1\n",
    "\n",
    "    plt.grid(True, ls='--', alpha=0.2, color='grey')\n",
    "    plt.title(\"K-Means Clustering on Random Data\")\n",
    "    plt.xlabel(f\"{x1}\")\n",
    "    plt.ylabel(f\"{x2}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942cf0a-f7b4-404d-8de0-b227515b605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering(km, \"feature1\", \"feature2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37759f45-e4b1-45cd-9676-8a8fdd5a6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow(data):\n",
    "    # K values range from 1 to 10\n",
    "    k_values = range(1, 10)\n",
    "    # formula for with-in cluster sum of squares\n",
    "    wcss_scores = [within_cluster_sum_of_squares(data, k) for k in k_values]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(k_values, wcss_scores, '-o', color='red')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.xlabel('Number of clusters k')\n",
    "    plt.ylabel('Within-Cluster Sum of Squares')\n",
    "    plt.xticks(k_values)\n",
    "    plt.grid(True, ls='--', alpha=0.2, color='grey')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087268a9-e040-403a-b73e-9b28890da9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f85f4-437a-41df-84be-61b147729c28",
   "metadata": {},
   "source": [
    "- Doe, J. (2021, April 5). [Create Your Own K-Means Clustering Algorithm in Python](https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670). Towards Data Science.\n",
    "- Smith, A. (2020, June 10). [K-Means Clustering from Scratch](https://www.askpython.com/python/examples/k-means-clustering-from-scratch). AskPython.\n",
    "- Lee, K. (2019, September 15). [Clustering Analysis: K-Means and Hierarchical Clustering by Hand and in R](https://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/). Stats and R.\n",
    "- Jones, B. (2022, February 28). [K-Means Clustering From Scratch](https://towardsdatascience.com/k-means-clustering-from-scratch-6a9d19cafc25). Towards Data Science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
